# encoding_node.py
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
import re

# ---------- READ INPUT ----------
if 'dm_inputdf' not in locals():
    raise NameError("dm_inputdf not defined")
data = dm_inputdf.copy()
print(f"âœ… Loaded data with shape: {data.shape}")

# ----------------------------------------------------
## ðŸ§¹ Pre-Encoding Missing Data Handling (New Section)
# ----------------------------------------------------
total_nans = data.isna().sum().sum()
if total_nans > 0:
    print(f"âš ï¸ Warning: Detected {total_nans} pre-existing NaN values. Imputing...")
    
    # Impute numeric NaNs with 0 (a standard approach before encoding/modeling)
    numeric_cols = data.select_dtypes(include=np.number).columns
    data[numeric_cols] = data[numeric_cols].fillna(0)
    
    # Impute object (categorical/text) NaNs with a placeholder string 'MISSING_CAT'
    object_cols = data.select_dtypes(include=['object']).columns
    data[object_cols] = data[object_cols].fillna('MISSING_CAT')
    
    print("âœ… Pre-encoding imputation complete. All NaNs handled.")
else:
    print("âœ… No pre-encoding missing values found.")

# ---------- DETECT CATEGORICAL AND TEXT ----------
categorical_cols = data.select_dtypes(include=['object']).columns.tolist()

TEXT_NAME_PATTERNS = ['desc','note','comment','text','remark','narrative','message','review']

def is_text_column(ser):
    ser = ser.dropna().astype(str)
    if ser.empty:
        return False
    avg_len = ser.str.len().mean()
    word_count = ser.str.split().str.len().mean()
    digits_ratio = ser.str.match(r'^\d+(\.\d+)?$').mean()
    return avg_len > 15 or word_count > 2 or digits_ratio < 0.5

text_cols = [c for c in categorical_cols if any(p in c.lower() for p in TEXT_NAME_PATTERNS) and is_text_column(data[c])]
cat_cols_to_encode = [c for c in categorical_cols if c not in text_cols]

# ----------------------------------------------------
# â¬‡ï¸ The rest of your code remains the same â¬‡ï¸
# ----------------------------------------------------

# ---------- ENCODE CATEGORICAL ----------
def safe_encode_categorical(df, col):
    n_unique = df[col].nunique(dropna=True)
    if n_unique <= 5:
        # One-Hot Encoding (for low cardinality)
        encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')
        # We must convert to string here in case 'MISSING_CAT' was introduced
        transformed = encoder.fit_transform(df[[col]].astype(str))
        new_cols = [f"{col}_{c}" for c in encoder.categories_[0][1:]]
        # Concat handles index alignment automatically
        df = pd.concat([df.drop(columns=[col]), pd.DataFrame(transformed, columns=new_cols, index=df.index)], axis=1)
    elif n_unique <= 50:
        # Label Encoding (for medium cardinality)
        encoder = LabelEncoder()
        df[col] = encoder.fit_transform(df[col].astype(str))
    else:
        # Frequency Encoding (for high cardinality)
        freq_map = df[col].value_counts(normalize=True)
        # Note: NaNs were handled upfront, so .fillna(0) here is mainly protective/redundant
        df[col] = df[col].map(freq_map).fillna(0)
    return df

for col in cat_cols_to_encode:
    data = safe_encode_categorical(data, col)

# ---------- ENCODE TEXT ----------
for text_col in text_cols:
    # Fill here is redundant due to pre-check, but kept for safety in the text data flow
    text_data = data[text_col].fillna('MISSING_CAT').astype(str) 
    try:
        vectorizer = TfidfVectorizer(max_features=100, min_df=2, max_df=0.8, stop_words='english', ngram_range=(1,2))
        tfidf_matrix = vectorizer.fit_transform(text_data)
        feature_names = [f"{text_col}_tfidf_{name}" for name in vectorizer.get_feature_names_out()]
        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=data.index)
        data = pd.concat([data.drop(columns=[text_col]), tfidf_df], axis=1)
    except Exception:
        # Fallback feature engineering if TF-IDF fails
        data[text_col + "_length"] = text_data.str.len()
        data[text_col + "_word_count"] = text_data.str.split().str.len()
        data[text_col + "_has_special"] = text_data.str.contains(r'[!@#$%^&*(),?":{}|<>]').astype(int)
        data.drop(columns=[text_col], inplace=True)

# ----------------------------------------------------
## ðŸ›‘ FINAL CHECK (Removed Unnecessary Imputation) ðŸ›‘
# ----------------------------------------------------
# Removed the redundant final imputation loop entirely. 
# The initial check handles pre-existing NaNs, and the encoding logic handles the rest.
# Any remaining NaNs at this point would typically indicate a different, critical issue.

# ---------- OUTPUT ----------
# We are intentionally setting the output to dm_outputdf. If the environment expects dm_scoreddf 
# (as in the previous run), the output wrapper code will fail again, but based on your previous 
# successful log, dm_outputdf should be sufficient for a standard Open Source Code node output.
dm_scoreddf = data.copy()
df_outputdf = data.copy()
print(f"âœ… Encoding completed, shape: {dm_scoreddf.shape}")
